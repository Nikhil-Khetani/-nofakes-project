{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_float_categorical_modified_11_01_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterfazekas1999/-nofakes-project/blob/main/train_float_categorical_modified_11_01_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTGJDkaxsrnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a20a6e-0e12-4206-e99d-9ae35cb72029"
      },
      "source": [
        "#code is used from these 3 repositories, have a look at them on GitHub or access the files from colab\n",
        "!git clone https://github.com/PeterWang512/FALdetector\n",
        "!git clone https://github.com/NVIDIA/flownet2-pytorch.git\n",
        "!git clone https://github.com/Kwanss/PCLNet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FALdetector'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Total 116 (delta 0), reused 0 (delta 0), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (116/116), 21.21 MiB | 43.00 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "Cloning into 'flownet2-pytorch'...\n",
            "remote: Enumerating objects: 557, done.\u001b[K\n",
            "remote: Total 557 (delta 0), reused 0 (delta 0), pack-reused 557\u001b[K\n",
            "Receiving objects: 100% (557/557), 6.28 MiB | 32.81 MiB/s, done.\n",
            "Resolving deltas: 100% (312/312), done.\n",
            "Cloning into 'PCLNet'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Total 51 (delta 0), reused 0 (delta 0), pack-reused 51\u001b[K\n",
            "Unpacking objects: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W8h8ElvvMC7"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Nov 25 10:32:09 2020\n",
        "\n",
        "@author: peter fazekas\n",
        "\"\"\"\n",
        "#import necessary modules and append paths \n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "sys.path.append(\"/content/FALdetector/networks/\")\n",
        "sys.path.append(\"/content/FALdetector/\")\n",
        "from drn_seg import DRNSeg\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss,BCELoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "sys.path.append(\"/content/flownet2-pytorch/\")\n",
        "import losses\n",
        "from losses import MultiScale,EPE\n",
        "sys.path.append(\"/content/PCLNet/Losses/\")\n",
        "sys.path.append(\"/content/PCLNet/models/\")\n",
        "from utils.tools import *\n",
        "from utils.visualize import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "# for reading and displaying images\n",
        "from skimage.io import imread\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from skimage import data, color\n",
        "from torchsummary import summary"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11kNl6zCuPPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87377767-612e-4fd6-b98a-5b509f6f38c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive\n",
        "!unzip /mydrive/nofakes/flow_pred_data/modified.zip -d modif\n",
        "!unzip /mydrive/nofakes/flow_pred_data/reference.zip -d ref\n",
        "!unzip /mydrive/nofakes/flow_pred_data/local_weight.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            " checkpoint_0.pt\t    local_detector_model_0.0.pt   NoFakes\n",
            " checkpoint.pt\t\t    local_detector_model_1.0.pt   NoFakes_FAL\n",
            " colab_2_train_flow.ipynb  'My Drive'\t\t\t 'Union Court Pictures'\n",
            "'Colab Notebooks'\t    nofakes\n",
            "Archive:  /mydrive/nofakes/flow_pred_data/modified.zip\n",
            "  inflating: modif/flickr_0500.png   \n",
            "  inflating: modif/flickr_0501.png   \n",
            "  inflating: modif/flickr_0502.png   \n",
            "  inflating: modif/flickr_0503.png   \n",
            "  inflating: modif/flickr_0504.png   \n",
            "  inflating: modif/flickr_0505.png   \n",
            "  inflating: modif/flickr_0506.png   \n",
            "  inflating: modif/flickr_0507.png   \n",
            "  inflating: modif/flickr_0508.png   \n",
            "  inflating: modif/flickr_0509.png   \n",
            "  inflating: modif/flickr_0510.png   \n",
            "  inflating: modif/flickr_0511.png   \n",
            "  inflating: modif/flickr_0512.png   \n",
            "  inflating: modif/flickr_0513.png   \n",
            "  inflating: modif/flickr_0514.png   \n",
            "  inflating: modif/flickr_0515.png   \n",
            "  inflating: modif/flickr_0516.png   \n",
            "  inflating: modif/flickr_0517.png   \n",
            "  inflating: modif/flickr_0518.png   \n",
            "  inflating: modif/flickr_0519.png   \n",
            "  inflating: modif/flickr_0520.png   \n",
            "  inflating: modif/flickr_0521.png   \n",
            "  inflating: modif/flickr_0522.png   \n",
            "  inflating: modif/flickr_0523.png   \n",
            "  inflating: modif/flickr_0524.png   \n",
            "  inflating: modif/flickr_0525.png   \n",
            "  inflating: modif/flickr_0526.png   \n",
            "  inflating: modif/flickr_0527.png   \n",
            "  inflating: modif/flickr_0528.png   \n",
            "  inflating: modif/flickr_0529.png   \n",
            "  inflating: modif/flickr_0530.png   \n",
            "  inflating: modif/flickr_0531.png   \n",
            "  inflating: modif/flickr_0532.png   \n",
            "  inflating: modif/flickr_0533.png   \n",
            "  inflating: modif/flickr_0534.png   \n",
            "  inflating: modif/flickr_0535.png   \n",
            "  inflating: modif/flickr_0536.png   \n",
            "  inflating: modif/flickr_0537.png   \n",
            "  inflating: modif/flickr_0538.png   \n",
            "  inflating: modif/flickr_0539.png   \n",
            "  inflating: modif/flickr_0540.png   \n",
            "  inflating: modif/flickr_0541.png   \n",
            "  inflating: modif/flickr_0542.png   \n",
            "  inflating: modif/flickr_0543.png   \n",
            "  inflating: modif/flickr_0544.png   \n",
            "  inflating: modif/flickr_0545.png   \n",
            "  inflating: modif/flickr_0546.png   \n",
            "  inflating: modif/flickr_0547.png   \n",
            "  inflating: modif/flickr_0548.png   \n",
            "  inflating: modif/flickr_0549.png   \n",
            "  inflating: modif/flickr_0550.png   \n",
            "  inflating: modif/flickr_0551.png   \n",
            "  inflating: modif/flickr_0552.png   \n",
            "  inflating: modif/flickr_0553.png   \n",
            "  inflating: modif/flickr_0554.png   \n",
            "  inflating: modif/flickr_0555.png   \n",
            "  inflating: modif/flickr_0556.png   \n",
            "  inflating: modif/flickr_0557.png   \n",
            "  inflating: modif/flickr_0558.png   \n",
            "  inflating: modif/flickr_0559.png   \n",
            "  inflating: modif/flickr_0560.png   \n",
            "  inflating: modif/flickr_0561.png   \n",
            "  inflating: modif/flickr_0562.png   \n",
            "  inflating: modif/flickr_0563.png   \n",
            "  inflating: modif/flickr_0564.png   \n",
            "  inflating: modif/flickr_0565.png   \n",
            "  inflating: modif/flickr_0566.png   \n",
            "  inflating: modif/flickr_0567.png   \n",
            "  inflating: modif/flickr_0568.png   \n",
            "  inflating: modif/flickr_0569.png   \n",
            "  inflating: modif/flickr_0570.png   \n",
            "  inflating: modif/flickr_0571.png   \n",
            "  inflating: modif/flickr_0572.png   \n",
            "  inflating: modif/flickr_0573.png   \n",
            "  inflating: modif/flickr_0574.png   \n",
            "  inflating: modif/flickr_0575.png   \n",
            "  inflating: modif/flickr_0576.png   \n",
            "  inflating: modif/flickr_0577.png   \n",
            "  inflating: modif/flickr_0578.png   \n",
            "  inflating: modif/flickr_0579.png   \n",
            "  inflating: modif/flickr_0580.png   \n",
            "  inflating: modif/flickr_0581.png   \n",
            "  inflating: modif/flickr_0582.png   \n",
            "  inflating: modif/flickr_0583.png   \n",
            "  inflating: modif/flickr_0584.png   \n",
            "  inflating: modif/flickr_0585.png   \n",
            "  inflating: modif/flickr_0586.png   \n",
            "  inflating: modif/flickr_0587.png   \n",
            "  inflating: modif/flickr_0588.png   \n",
            "  inflating: modif/flickr_0589.png   \n",
            "  inflating: modif/flickr_0590.png   \n",
            "  inflating: modif/flickr_0591.png   \n",
            "  inflating: modif/flickr_0592.png   \n",
            "  inflating: modif/flickr_0593.png   \n",
            "  inflating: modif/flickr_0594.png   \n",
            "  inflating: modif/flickr_0595.png   \n",
            "  inflating: modif/flickr_0596.png   \n",
            "  inflating: modif/flickr_0597.png   \n",
            "  inflating: modif/flickr_0598.png   \n",
            "  inflating: modif/flickr_0599.png   \n",
            "  inflating: modif/flickr_0600.png   \n",
            "Archive:  /mydrive/nofakes/flow_pred_data/reference.zip\n",
            "  inflating: ref/flickr_0500.png     \n",
            "  inflating: ref/flickr_0501.png     \n",
            "  inflating: ref/flickr_0502.png     \n",
            "  inflating: ref/flickr_0503.png     \n",
            "  inflating: ref/flickr_0504.png     \n",
            "  inflating: ref/flickr_0505.png     \n",
            "  inflating: ref/flickr_0506.png     \n",
            "  inflating: ref/flickr_0507.png     \n",
            "  inflating: ref/flickr_0508.png     \n",
            "  inflating: ref/flickr_0509.png     \n",
            "  inflating: ref/flickr_0510.png     \n",
            "  inflating: ref/flickr_0511.png     \n",
            "  inflating: ref/flickr_0512.png     \n",
            "  inflating: ref/flickr_0513.png     \n",
            "  inflating: ref/flickr_0514.png     \n",
            "  inflating: ref/flickr_0515.png     \n",
            "  inflating: ref/flickr_0516.png     \n",
            "  inflating: ref/flickr_0517.png     \n",
            "  inflating: ref/flickr_0518.png     \n",
            "  inflating: ref/flickr_0519.png     \n",
            "  inflating: ref/flickr_0520.png     \n",
            "  inflating: ref/flickr_0521.png     \n",
            "  inflating: ref/flickr_0522.png     \n",
            "  inflating: ref/flickr_0523.png     \n",
            "  inflating: ref/flickr_0524.png     \n",
            "  inflating: ref/flickr_0525.png     \n",
            "  inflating: ref/flickr_0526.png     \n",
            "  inflating: ref/flickr_0527.png     \n",
            "  inflating: ref/flickr_0528.png     \n",
            "  inflating: ref/flickr_0529.png     \n",
            "  inflating: ref/flickr_0530.png     \n",
            "  inflating: ref/flickr_0531.png     \n",
            "  inflating: ref/flickr_0532.png     \n",
            "  inflating: ref/flickr_0533.png     \n",
            "  inflating: ref/flickr_0534.png     \n",
            "  inflating: ref/flickr_0535.png     \n",
            "  inflating: ref/flickr_0536.png     \n",
            "  inflating: ref/flickr_0537.png     \n",
            "  inflating: ref/flickr_0538.png     \n",
            "  inflating: ref/flickr_0539.png     \n",
            "  inflating: ref/flickr_0540.png     \n",
            "  inflating: ref/flickr_0541.png     \n",
            "  inflating: ref/flickr_0542.png     \n",
            "  inflating: ref/flickr_0543.png     \n",
            "  inflating: ref/flickr_0544.png     \n",
            "  inflating: ref/flickr_0545.png     \n",
            "  inflating: ref/flickr_0546.png     \n",
            "  inflating: ref/flickr_0547.png     \n",
            "  inflating: ref/flickr_0548.png     \n",
            "  inflating: ref/flickr_0549.png     \n",
            "  inflating: ref/flickr_0550.png     \n",
            "  inflating: ref/flickr_0551.png     \n",
            "  inflating: ref/flickr_0552.png     \n",
            "  inflating: ref/flickr_0553.png     \n",
            "  inflating: ref/flickr_0554.png     \n",
            "  inflating: ref/flickr_0555.png     \n",
            "  inflating: ref/flickr_0556.png     \n",
            "  inflating: ref/flickr_0557.png     \n",
            "  inflating: ref/flickr_0558.png     \n",
            "  inflating: ref/flickr_0559.png     \n",
            "  inflating: ref/flickr_0560.png     \n",
            "  inflating: ref/flickr_0561.png     \n",
            "  inflating: ref/flickr_0562.png     \n",
            "  inflating: ref/flickr_0563.png     \n",
            "  inflating: ref/flickr_0564.png     \n",
            "  inflating: ref/flickr_0565.png     \n",
            "  inflating: ref/flickr_0566.png     \n",
            "  inflating: ref/flickr_0567.png     \n",
            "  inflating: ref/flickr_0568.png     \n",
            "  inflating: ref/flickr_0569.png     \n",
            "  inflating: ref/flickr_0570.png     \n",
            "  inflating: ref/flickr_0571.png     \n",
            "  inflating: ref/flickr_0572.png     \n",
            "  inflating: ref/flickr_0573.png     \n",
            "  inflating: ref/flickr_0574.png     \n",
            "  inflating: ref/flickr_0575.png     \n",
            "  inflating: ref/flickr_0576.png     \n",
            "  inflating: ref/flickr_0577.png     \n",
            "  inflating: ref/flickr_0578.png     \n",
            "  inflating: ref/flickr_0579.png     \n",
            "  inflating: ref/flickr_0580.png     \n",
            "  inflating: ref/flickr_0581.png     \n",
            "  inflating: ref/flickr_0582.png     \n",
            "  inflating: ref/flickr_0583.png     \n",
            "  inflating: ref/flickr_0584.png     \n",
            "  inflating: ref/flickr_0585.png     \n",
            "  inflating: ref/flickr_0586.png     \n",
            "  inflating: ref/flickr_0587.png     \n",
            "  inflating: ref/flickr_0588.png     \n",
            "  inflating: ref/flickr_0589.png     \n",
            "  inflating: ref/flickr_0590.png     \n",
            "  inflating: ref/flickr_0591.png     \n",
            "  inflating: ref/flickr_0592.png     \n",
            "  inflating: ref/flickr_0593.png     \n",
            "  inflating: ref/flickr_0594.png     \n",
            "  inflating: ref/flickr_0595.png     \n",
            "  inflating: ref/flickr_0596.png     \n",
            "  inflating: ref/flickr_0597.png     \n",
            "  inflating: ref/flickr_0598.png     \n",
            "  inflating: ref/flickr_0599.png     \n",
            "  inflating: ref/flickr_0600.png     \n",
            "Archive:  /mydrive/nofakes/flow_pred_data/local_weight.zip\n",
            "  inflating: local.pth               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCc_Sot-srnw",
        "outputId": "44a23064-dfe2-4fd5-88dc-7bd52ca30de4"
      },
      "source": [
        "pathM =r\"/content/modif\"\n",
        "path =r\"/content/ref\"\n",
        "\n",
        "#if you guys can come up with a more efficient way to do this go ahead.\n",
        "height =400\n",
        "width=400\n",
        "#variable to control how many training examples to import\n",
        "train_size =5\n",
        "filenames = []\n",
        "\n",
        "#Redo this part.... it doesnt import training images in order, I have printed out the filenames imported for convenience\n",
        "def createTrain(path,n_images):\n",
        "    arr=[]\n",
        "    counter =0\n",
        "    for r,d,f in os.walk(path):\n",
        "        for _file in f:\n",
        "          #make sure to change to correct image format .jpg or .png etc...\n",
        "            if '.png' in _file:\n",
        "                #print(_file)\n",
        "                img = cv2.imread(r\"\"+path+\"/\"+str(_file))\n",
        "                filenames.append(path+\"/\"+str(_file))\n",
        "                #keep image dimensions at 500 for now\n",
        "                img = cv2.resize(img, (width,height), interpolation = cv2.INTER_AREA)\n",
        "                #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "                arr.append(img)\n",
        "                counter+=1\n",
        "                #terminate once we imported the desired number of images\n",
        "                if(counter==n_images):\n",
        "                    return np.array(arr)\n",
        "                    \n",
        "\n",
        "X_ref=createTrain(path,train_size)\n",
        "X_mod = createTrain(pathM,train_size)\n",
        "\n",
        "print(\"the filenames are\",filenames)\n",
        "shape = X_ref.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the filenames are ['/content/ref/flickr_0587.png', '/content/ref/flickr_0510.png', '/content/ref/flickr_0541.png', '/content/ref/flickr_0505.png', '/content/ref/flickr_0553.png', '/content/modif/flickr_0587.png', '/content/modif/flickr_0510.png', '/content/modif/flickr_0541.png', '/content/modif/flickr_0505.png', '/content/modif/flickr_0553.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn4RUUb4srnx",
        "outputId": "3a7bfb8d-aba9-4043-c78b-1404870c0945"
      },
      "source": [
        "flow_arr =[]\n",
        "#calcOptical gives an error if I input all training data at once (doesnt seem very efficient atm)\n",
        "for i in range(shape[0]):\n",
        "    flow = cv2.calcOpticalFlowFarneback(cv2.cvtColor(X_ref[i],cv2.COLOR_BGR2GRAY), cv2.cvtColor(X_mod[i],cv2.COLOR_BGR2GRAY),None, 0.5, 3, 4, 2, 3, 1.2, 0)\n",
        "    flow_arr.append(flow)    \n",
        "print(X_mod.shape)\n",
        "flow_arr = np.array(flow_arr)\n",
        "mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
        "print(mag.shape)\n",
        "path = \"tester_before_train.png\"\n",
        "tester = save_heatmap_cv(X_mod[0], mag, path)\n",
        "#print(flow)\n",
        "print(\"flow arr has shape\",flow_arr.shape)\n",
        "#the code below is pretty similar to how the binary classifier is trained with a few changes \n",
        "#since the output shape is (2,height,width) for the vector field"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 400, 400, 3)\n",
            "(400, 400)\n",
            "flow arr has shape (5, 400, 400, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WaL8ji9pVCE"
      },
      "source": [
        "**Discretize the flow fields**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKrXUh7npF0J",
        "outputId": "40bf8db8-d13c-4edd-b691-da9e5e386e92"
      },
      "source": [
        "print(\"maximum flow value is: \",np.max(flow_arr))\n",
        "print(\"minimum flow value is: \",np.min(flow_arr))\n",
        "categorical_flow =[]\n",
        "dic ={}\n",
        "inv_dic={}\n",
        "counter =0\n",
        "#create a placeholder for the (u,v)->class pairs, hashmap is the easiest as its O(1) lookup time for the values\n",
        "#Note when obtaining flow values from classes it takes O(n) time to look up each one, not very efficient so\n",
        "#maybe re-do this part so that the hashmap key-value pairs are {class_pairs:(u,v)}\n",
        "\n",
        "#max and minimum allowed flow values\n",
        "max_f =5\n",
        "min_f=-5\n",
        "#fill up hashmap with values\n",
        "for i in range(min_f,max_f+1):\n",
        "  for j in range(min_f,max_f+1):\n",
        "    dic[i,j]=counter\n",
        "    inv_dic[counter]=[i,j]\n",
        "    counter+=1\n",
        "print(len(dic))\n",
        "print(\"this is what the hashmap looks like\",dic)\n",
        "print(\"this is what the inverse looks like\",inv_dic)\n",
        "dimens= flow_arr.shape\n",
        "\n",
        "#categorise the flow into distinct values\n",
        "for flows in flow_arr:\n",
        "  temp_flow =np.zeros((dimens[1],dimens[2]))\n",
        "  print(temp_flow.shape)\n",
        "  for i in range(dimens[1]):\n",
        "    for j in range(dimens[2]):\n",
        "      value = flows[i][j]\n",
        "      #makes sure we are not going over the max and min values \n",
        "      if (value[1]>max_f or value[1]<min_f or value[0]>max_f or value[0]<min_f):\n",
        "        continue\n",
        "      temp_flow[i][j]=dic[int(value[0]),int(value[1])]\n",
        "      \n",
        "  categorical_flow.append(temp_flow)\n",
        "categorical_flow = np.array(categorical_flow)\n",
        "#print(categorical_flow.shape)\n",
        "#print(categorical_flow[0][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum flow value is:  26.950367\n",
            "minimum flow value is:  -9.915729\n",
            "121\n",
            "this is what the hashmap looks like {(-5, -5): 0, (-5, -4): 1, (-5, -3): 2, (-5, -2): 3, (-5, -1): 4, (-5, 0): 5, (-5, 1): 6, (-5, 2): 7, (-5, 3): 8, (-5, 4): 9, (-5, 5): 10, (-4, -5): 11, (-4, -4): 12, (-4, -3): 13, (-4, -2): 14, (-4, -1): 15, (-4, 0): 16, (-4, 1): 17, (-4, 2): 18, (-4, 3): 19, (-4, 4): 20, (-4, 5): 21, (-3, -5): 22, (-3, -4): 23, (-3, -3): 24, (-3, -2): 25, (-3, -1): 26, (-3, 0): 27, (-3, 1): 28, (-3, 2): 29, (-3, 3): 30, (-3, 4): 31, (-3, 5): 32, (-2, -5): 33, (-2, -4): 34, (-2, -3): 35, (-2, -2): 36, (-2, -1): 37, (-2, 0): 38, (-2, 1): 39, (-2, 2): 40, (-2, 3): 41, (-2, 4): 42, (-2, 5): 43, (-1, -5): 44, (-1, -4): 45, (-1, -3): 46, (-1, -2): 47, (-1, -1): 48, (-1, 0): 49, (-1, 1): 50, (-1, 2): 51, (-1, 3): 52, (-1, 4): 53, (-1, 5): 54, (0, -5): 55, (0, -4): 56, (0, -3): 57, (0, -2): 58, (0, -1): 59, (0, 0): 60, (0, 1): 61, (0, 2): 62, (0, 3): 63, (0, 4): 64, (0, 5): 65, (1, -5): 66, (1, -4): 67, (1, -3): 68, (1, -2): 69, (1, -1): 70, (1, 0): 71, (1, 1): 72, (1, 2): 73, (1, 3): 74, (1, 4): 75, (1, 5): 76, (2, -5): 77, (2, -4): 78, (2, -3): 79, (2, -2): 80, (2, -1): 81, (2, 0): 82, (2, 1): 83, (2, 2): 84, (2, 3): 85, (2, 4): 86, (2, 5): 87, (3, -5): 88, (3, -4): 89, (3, -3): 90, (3, -2): 91, (3, -1): 92, (3, 0): 93, (3, 1): 94, (3, 2): 95, (3, 3): 96, (3, 4): 97, (3, 5): 98, (4, -5): 99, (4, -4): 100, (4, -3): 101, (4, -2): 102, (4, -1): 103, (4, 0): 104, (4, 1): 105, (4, 2): 106, (4, 3): 107, (4, 4): 108, (4, 5): 109, (5, -5): 110, (5, -4): 111, (5, -3): 112, (5, -2): 113, (5, -1): 114, (5, 0): 115, (5, 1): 116, (5, 2): 117, (5, 3): 118, (5, 4): 119, (5, 5): 120}\n",
            "this is what the inverse looks like {0: [-5, -5], 1: [-5, -4], 2: [-5, -3], 3: [-5, -2], 4: [-5, -1], 5: [-5, 0], 6: [-5, 1], 7: [-5, 2], 8: [-5, 3], 9: [-5, 4], 10: [-5, 5], 11: [-4, -5], 12: [-4, -4], 13: [-4, -3], 14: [-4, -2], 15: [-4, -1], 16: [-4, 0], 17: [-4, 1], 18: [-4, 2], 19: [-4, 3], 20: [-4, 4], 21: [-4, 5], 22: [-3, -5], 23: [-3, -4], 24: [-3, -3], 25: [-3, -2], 26: [-3, -1], 27: [-3, 0], 28: [-3, 1], 29: [-3, 2], 30: [-3, 3], 31: [-3, 4], 32: [-3, 5], 33: [-2, -5], 34: [-2, -4], 35: [-2, -3], 36: [-2, -2], 37: [-2, -1], 38: [-2, 0], 39: [-2, 1], 40: [-2, 2], 41: [-2, 3], 42: [-2, 4], 43: [-2, 5], 44: [-1, -5], 45: [-1, -4], 46: [-1, -3], 47: [-1, -2], 48: [-1, -1], 49: [-1, 0], 50: [-1, 1], 51: [-1, 2], 52: [-1, 3], 53: [-1, 4], 54: [-1, 5], 55: [0, -5], 56: [0, -4], 57: [0, -3], 58: [0, -2], 59: [0, -1], 60: [0, 0], 61: [0, 1], 62: [0, 2], 63: [0, 3], 64: [0, 4], 65: [0, 5], 66: [1, -5], 67: [1, -4], 68: [1, -3], 69: [1, -2], 70: [1, -1], 71: [1, 0], 72: [1, 1], 73: [1, 2], 74: [1, 3], 75: [1, 4], 76: [1, 5], 77: [2, -5], 78: [2, -4], 79: [2, -3], 80: [2, -2], 81: [2, -1], 82: [2, 0], 83: [2, 1], 84: [2, 2], 85: [2, 3], 86: [2, 4], 87: [2, 5], 88: [3, -5], 89: [3, -4], 90: [3, -3], 91: [3, -2], 92: [3, -1], 93: [3, 0], 94: [3, 1], 95: [3, 2], 96: [3, 3], 97: [3, 4], 98: [3, 5], 99: [4, -5], 100: [4, -4], 101: [4, -3], 102: [4, -2], 103: [4, -1], 104: [4, 0], 105: [4, 1], 106: [4, 2], 107: [4, 3], 108: [4, 4], 109: [4, 5], 110: [5, -5], 111: [5, -4], 112: [5, -3], 113: [5, -2], 114: [5, -1], 115: [5, 0], 116: [5, 1], 117: [5, 2], 118: [5, 3], 119: [5, 4], 120: [5, 5]}\n",
            "(400, 400)\n",
            "(400, 400)\n",
            "(400, 400)\n",
            "(400, 400)\n",
            "(400, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vqlb8wk3mrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db94ea7-0ae9-477d-ed70-1c69c8986d07"
      },
      "source": [
        "#Sanity check to make sure shapes add up\n",
        "\n",
        "print(\"ytrain has shape\",categorical_flow.shape)\n",
        "print(\"xtrain has shape\",X_mod.shape)\n",
        "#print(categorical_flow[0][100])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ytrain has shape (5, 400, 400)\n",
            "xtrain has shape (5, 400, 400, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aphClU4NrJ3E"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM9U8BGOrJEv"
      },
      "source": [
        "**make sure flow is set up properly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYz6MyxrHrx"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1WjTZ36yieZ",
        "outputId": "c3a97e6c-0a41-4571-f48e-9cd34d0f0be7"
      },
      "source": [
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IGPecamsrnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f952f718-c440-41b8-c26f-afcf5bb2b372"
      },
      "source": [
        "#check if GPU is available\n",
        "cuda0 = torch.device('cuda:0')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:{}'.format(\"cuda:0\")\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "\n",
        "model = DRNSeg(2)\n",
        "device = torch.device(cuda0)\n",
        "#state_dict = torch.load(\"local.pth\", map_location=device)\n",
        "#model.load_state_dict(state_dict['model'])\n",
        "model.to(device)\n",
        "#summary(model, (3, height, width))\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DRNSeg(\n",
              "  (base): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (seg): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (up): ConvTranspose2d(2, 2, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4), groups=2, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNP5FvjtW9A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a250917-da8f-4e6a-f4e3-bf77f8c85c4c"
      },
      "source": [
        "temp = torch.randn(1, 3, 200, 200)\n",
        "temp = temp.cuda()\n",
        "sizes =[2,4,16,32,64,121]\n",
        "# Let's print it\n",
        "shape=model(temp).shape\n",
        "print(shape)\n",
        "\n",
        "#leave it out for now as I am using the DRNSeg(121)\n",
        "if False:\n",
        "  perPix_model = nn.Sequential(model,nn.ReLU(),\n",
        "                              nn.Conv2d(sizes[0],sizes[1],kernel_size=(1,1)).cuda(),nn.ReLU(),\n",
        "                              nn.Conv2d(sizes[1],sizes[2],kernel_size=(1,1)).cuda(),nn.ReLU(),\n",
        "                              nn.Conv2d(sizes[2],sizes[3],kernel_size=(1,1)).cuda(),nn.ReLU(),\n",
        "                               nn.Conv2d(sizes[3],sizes[4],kernel_size=(1,1)).cuda(),nn.ReLU(),\n",
        "                               nn.Conv2d(sizes[4],sizes[5],kernel_size=(1,1)).cuda(),\n",
        "                              nn.Tanh()\n",
        "                              )\n",
        "#just add a tanh activation onto it\n",
        "perPix_model = DRNSeg(len(dic)).to(device)\n",
        "summary(perPix_model, (3, height, width))\n",
        "perPix_model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 200, 200])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 400, 400]           2,352\n",
            "       BatchNorm2d-2         [-1, 16, 400, 400]              32\n",
            "              ReLU-3         [-1, 16, 400, 400]               0\n",
            "            Conv2d-4         [-1, 16, 400, 400]           2,304\n",
            "       BatchNorm2d-5         [-1, 16, 400, 400]              32\n",
            "              ReLU-6         [-1, 16, 400, 400]               0\n",
            "            Conv2d-7         [-1, 16, 400, 400]           2,304\n",
            "       BatchNorm2d-8         [-1, 16, 400, 400]              32\n",
            "              ReLU-9         [-1, 16, 400, 400]               0\n",
            "       BasicBlock-10         [-1, 16, 400, 400]               0\n",
            "           Conv2d-11         [-1, 32, 200, 200]           4,608\n",
            "      BatchNorm2d-12         [-1, 32, 200, 200]              64\n",
            "             ReLU-13         [-1, 32, 200, 200]               0\n",
            "           Conv2d-14         [-1, 32, 200, 200]           9,216\n",
            "      BatchNorm2d-15         [-1, 32, 200, 200]              64\n",
            "           Conv2d-16         [-1, 32, 200, 200]             512\n",
            "      BatchNorm2d-17         [-1, 32, 200, 200]              64\n",
            "             ReLU-18         [-1, 32, 200, 200]               0\n",
            "       BasicBlock-19         [-1, 32, 200, 200]               0\n",
            "           Conv2d-20         [-1, 64, 100, 100]          18,432\n",
            "      BatchNorm2d-21         [-1, 64, 100, 100]             128\n",
            "             ReLU-22         [-1, 64, 100, 100]               0\n",
            "           Conv2d-23         [-1, 64, 100, 100]          36,864\n",
            "      BatchNorm2d-24         [-1, 64, 100, 100]             128\n",
            "           Conv2d-25         [-1, 64, 100, 100]           2,048\n",
            "      BatchNorm2d-26         [-1, 64, 100, 100]             128\n",
            "             ReLU-27         [-1, 64, 100, 100]               0\n",
            "       BasicBlock-28         [-1, 64, 100, 100]               0\n",
            "           Conv2d-29         [-1, 64, 100, 100]          36,864\n",
            "      BatchNorm2d-30         [-1, 64, 100, 100]             128\n",
            "             ReLU-31         [-1, 64, 100, 100]               0\n",
            "           Conv2d-32         [-1, 64, 100, 100]          36,864\n",
            "      BatchNorm2d-33         [-1, 64, 100, 100]             128\n",
            "             ReLU-34         [-1, 64, 100, 100]               0\n",
            "       BasicBlock-35         [-1, 64, 100, 100]               0\n",
            "           Conv2d-36          [-1, 128, 50, 50]          73,728\n",
            "      BatchNorm2d-37          [-1, 128, 50, 50]             256\n",
            "             ReLU-38          [-1, 128, 50, 50]               0\n",
            "           Conv2d-39          [-1, 128, 50, 50]         147,456\n",
            "      BatchNorm2d-40          [-1, 128, 50, 50]             256\n",
            "           Conv2d-41          [-1, 128, 50, 50]           8,192\n",
            "      BatchNorm2d-42          [-1, 128, 50, 50]             256\n",
            "             ReLU-43          [-1, 128, 50, 50]               0\n",
            "       BasicBlock-44          [-1, 128, 50, 50]               0\n",
            "           Conv2d-45          [-1, 128, 50, 50]         147,456\n",
            "      BatchNorm2d-46          [-1, 128, 50, 50]             256\n",
            "             ReLU-47          [-1, 128, 50, 50]               0\n",
            "           Conv2d-48          [-1, 128, 50, 50]         147,456\n",
            "      BatchNorm2d-49          [-1, 128, 50, 50]             256\n",
            "             ReLU-50          [-1, 128, 50, 50]               0\n",
            "       BasicBlock-51          [-1, 128, 50, 50]               0\n",
            "           Conv2d-52          [-1, 256, 50, 50]         294,912\n",
            "      BatchNorm2d-53          [-1, 256, 50, 50]             512\n",
            "             ReLU-54          [-1, 256, 50, 50]               0\n",
            "           Conv2d-55          [-1, 256, 50, 50]         589,824\n",
            "      BatchNorm2d-56          [-1, 256, 50, 50]             512\n",
            "           Conv2d-57          [-1, 256, 50, 50]          32,768\n",
            "      BatchNorm2d-58          [-1, 256, 50, 50]             512\n",
            "             ReLU-59          [-1, 256, 50, 50]               0\n",
            "       BasicBlock-60          [-1, 256, 50, 50]               0\n",
            "           Conv2d-61          [-1, 256, 50, 50]         589,824\n",
            "      BatchNorm2d-62          [-1, 256, 50, 50]             512\n",
            "             ReLU-63          [-1, 256, 50, 50]               0\n",
            "           Conv2d-64          [-1, 256, 50, 50]         589,824\n",
            "      BatchNorm2d-65          [-1, 256, 50, 50]             512\n",
            "             ReLU-66          [-1, 256, 50, 50]               0\n",
            "       BasicBlock-67          [-1, 256, 50, 50]               0\n",
            "           Conv2d-68          [-1, 512, 50, 50]       1,179,648\n",
            "      BatchNorm2d-69          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-70          [-1, 512, 50, 50]               0\n",
            "           Conv2d-71          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-72          [-1, 512, 50, 50]           1,024\n",
            "           Conv2d-73          [-1, 512, 50, 50]         131,072\n",
            "      BatchNorm2d-74          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-75          [-1, 512, 50, 50]               0\n",
            "       BasicBlock-76          [-1, 512, 50, 50]               0\n",
            "           Conv2d-77          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-78          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-79          [-1, 512, 50, 50]               0\n",
            "           Conv2d-80          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-81          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-82          [-1, 512, 50, 50]               0\n",
            "       BasicBlock-83          [-1, 512, 50, 50]               0\n",
            "           Conv2d-84          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-85          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-86          [-1, 512, 50, 50]               0\n",
            "           Conv2d-87          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-88          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-89          [-1, 512, 50, 50]               0\n",
            "       BasicBlock-90          [-1, 512, 50, 50]               0\n",
            "           Conv2d-91          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-92          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-93          [-1, 512, 50, 50]               0\n",
            "           Conv2d-94          [-1, 512, 50, 50]       2,359,296\n",
            "      BatchNorm2d-95          [-1, 512, 50, 50]           1,024\n",
            "             ReLU-96          [-1, 512, 50, 50]               0\n",
            "       BasicBlock-97          [-1, 512, 50, 50]               0\n",
            "           Conv2d-98          [-1, 121, 50, 50]          62,073\n",
            "  ConvTranspose2d-99        [-1, 121, 400, 400]          30,976\n",
            "================================================================\n",
            "Total params: 20,706,633\n",
            "Trainable params: 20,675,657\n",
            "Non-trainable params: 30,976\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.83\n",
            "Forward/backward pass size (MB): 921.50\n",
            "Params size (MB): 78.99\n",
            "Estimated Total Size (MB): 1002.32\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DRNSeg(\n",
              "  (base): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (seg): Conv2d(512, 121, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (up): ConvTranspose2d(121, 121, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4), groups=121, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtVAZde-vW3w"
      },
      "source": [
        "**Training the Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1fXnALpsrnx",
        "outputId": "bc0f2495-467f-4f43-b4a9-1f4af706b103"
      },
      "source": [
        "#changed crossentropy to MSE since cross entropy needs classes, we arent doing classification\n",
        "#other losses might work much better \n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "optimizer = Adam(model.parameters(),lr=0.07) \n",
        "criterion =torch.nn.MSELoss()\n",
        "criterion_classif = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#criterion = MultiScale(model)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  perPix_model =perPix_model.cuda()\n",
        "  criterion = criterion.cuda()\n",
        "  criterion_classif=criterion_classif.cuda()\n",
        "#convert to torch format\n",
        "train_x = X_mod\n",
        "train_x_orig=X_ref\n",
        "train_x = train_x.reshape(train_size,3, height, width)\n",
        "train_x_orig = train_x_orig.reshape(train_size,3, height, width)\n",
        "print(\"train_x has shape\",train_x.shape)\n",
        "train_x  = torch.from_numpy(train_x)\n",
        "\n",
        "#train_x_orig  = torch.from_numpy(train_x_orig).float()\n",
        "\n",
        "\n",
        "# converting the target into torch format\n",
        "train_y=flow_arr\n",
        "cat_shape = categorical_flow.shape\n",
        "categorical_label=torch.from_numpy(categorical_flow)\n",
        "train_y = train_y.reshape(train_size,2,height,width)\n",
        "print(\"train_y has shape: \",train_y.shape)\n",
        "train_y = torch.from_numpy(train_y).float()\n",
        "print(\"xtrain shape\",train_x.shape)\n",
        "print(\"y shape\",categorical_label.shape)\n",
        "\n",
        "\n",
        "#MAIN CHANGE, dataloader so that batches can be trained, use batch size of 1 and then multiple pictures can be trained\n",
        "my_dataset = TensorDataset(train_x,categorical_label) # create your datset\n",
        "b_size =1\n",
        "my_dataloader = DataLoader(my_dataset,batch_size=b_size)\n",
        "\n",
        "\n",
        "\n",
        "#this part of the code trains the classifier called perPix_model\n",
        "def train_classif(epoch,b_size):\n",
        "    correct =0\n",
        "    #iterate over batches\n",
        "    for idx,(x_t,y_t) in enumerate(my_dataloader):\n",
        "      perPix_model.train()\n",
        "      tr_loss = 0\n",
        "      \n",
        "      # getting the training set\n",
        "      x_train, y_categorical = Variable(x_t), Variable(y_t)\n",
        "      \n",
        "      # converting the data into GPU format\n",
        "      if torch.cuda.is_available():\n",
        "          x_train = x_train.cuda()\n",
        "          y_categorical = y_categorical.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # prediction for training and validation set\n",
        "      output_train = perPix_model(x_train.float())\n",
        "      \n",
        "      \n",
        "      #print(\"y has shape\",y_categorical.shape)\n",
        "      label_output = torch.argmax(output_train,dim=1)\n",
        "      #print(\"label out shape\",label_output.shape)\n",
        "      #print(\"output shape\",output_train.shape)\n",
        "      Loss = criterion_classif(output_train, y_categorical.long())\n",
        "\n",
        "\n",
        "      #print(label_output)\n",
        "      #print(\"label output has shape\",label_output.shape)\n",
        "      correct = (label_output==y_categorical).sum().float()/(height*width*b_size)\n",
        "      \n",
        "\n",
        "      total = train_size\n",
        "      \n",
        "      #print(Loss)\n",
        "      train_losses.append(Loss)\n",
        "        \n",
        "        # computing the updated weights of all the model parameters\n",
        "      Loss.backward()\n",
        "      optimizer.step()\n",
        "      tr_loss = Loss.item()\n",
        "      if epoch%10 == 0:\n",
        "          # printing the validation loss\n",
        "          print('Epoch : ',epoch+1, \"for Batch:\",idx, '\\t', 'loss :',Loss.item(),\"accuracy: \",correct.item())\n",
        "      if epoch%1000 == 0:\n",
        "        checkpoint_path = \"gdrive/MyDrive/local_detector_model_{}.pt\".format(int(epoch/1000))\n",
        "        torch.save({\n",
        "            'checkpoint_epoch': checkpoint_epoch+n_epochs_to_train,\n",
        "            'model_state_dict': perPix_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_losses': train_losses\n",
        "            }, checkpoint_path)\n",
        "      print(checkpoint_epoch+n_epochs_to_train)\n",
        "    \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x has shape (5, 3, 400, 400)\n",
            "train_y has shape:  (5, 2, 400, 400)\n",
            "xtrain shape torch.Size([5, 3, 400, 400])\n",
            "y shape torch.Size([5, 400, 400])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD_FcDy6ueFx",
        "outputId": "ce34c7b1-ae23-4d3b-c622-dd8283562fb6"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\r\n",
        "input = torch.randn(3, 5, requires_grad=True)\r\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\r\n",
        "output = loss(input, target)\r\n",
        "print(input.shape)\r\n",
        "print(target.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n",
            "torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUxCx2RD4s8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f7a50a-7a9f-45ed-93b4-89f247af9aaf"
      },
      "source": [
        "#Check for existing checkpoint in content/, train additional epochs if found\n",
        "n_epochs_to_train = 4000\n",
        "train_losses =[]\n",
        "correct =0\n",
        "\n",
        "#same parameters the FAL paper have used\n",
        "optimizer = Adam(perPix_model.parameters(),lr=0.0001,betas =(0.9,0.999))\n",
        "\n",
        "for i in reversed(range(100)):\n",
        "  checkpoint_path = \"gdrive/MyDrive/local_detector_model_{}.pt\".format(i*1000)\n",
        "  if os.path.exists(checkpoint_path):\n",
        "      checkpoint = torch.load(checkpoint_path)\n",
        "      perPix_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      checkpoint_epoch = checkpoint['checkpoint_epoch']\n",
        "      train_losses = checkpoint['train_losses']\n",
        "      print(\"checkpoint loaded\")\n",
        "      print(\"checkpoint epoch:\" + str(checkpoint_epoch))\n",
        "      break\n",
        "  if i==0:\n",
        "    checkpoint_epoch=0\n",
        "    print(\"No existing checkpoint found\")\n",
        "\n",
        "\n",
        "for i in range(n_epochs_to_train):\n",
        "    train_classif(checkpoint_epoch+i,b_size)\n",
        "\n",
        "print(\"In this run, another \"+str(n_epochs_to_train)+\" epochs were trained\")\n",
        "print(\"Total epochs model has been trained upon:\" + str(checkpoint_epoch+n_epochs_to_train))\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No existing checkpoint found\n",
            "Epoch :  1 for Batch: 0 \t loss : 0.25613027811050415 accuracy:  0.9598999619483948\n",
            "200\n",
            "Epoch :  1 for Batch: 1 \t loss : 0.030037319287657738 accuracy:  0.9889812469482422\n",
            "200\n",
            "Epoch :  1 for Batch: 2 \t loss : 0.07471411675214767 accuracy:  0.9872750043869019\n",
            "200\n",
            "Epoch :  1 for Batch: 3 \t loss : 0.04764079302549362 accuracy:  0.9828436970710754\n",
            "200\n",
            "Epoch :  1 for Batch: 4 \t loss : 0.089988112449646 accuracy:  0.9700062274932861\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  11 for Batch: 0 \t loss : 0.042672354727983475 accuracy:  0.9851686954498291\n",
            "200\n",
            "Epoch :  11 for Batch: 1 \t loss : 0.027892226353287697 accuracy:  0.9894999861717224\n",
            "200\n",
            "Epoch :  11 for Batch: 2 \t loss : 0.04001754894852638 accuracy:  0.9895562529563904\n",
            "200\n",
            "Epoch :  11 for Batch: 3 \t loss : 0.03538632020354271 accuracy:  0.98685622215271\n",
            "200\n",
            "Epoch :  11 for Batch: 4 \t loss : 0.06692344695329666 accuracy:  0.9744186997413635\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  21 for Batch: 0 \t loss : 0.03729400411248207 accuracy:  0.9861062169075012\n",
            "200\n",
            "Epoch :  21 for Batch: 1 \t loss : 0.024163611233234406 accuracy:  0.9904499650001526\n",
            "200\n",
            "Epoch :  21 for Batch: 2 \t loss : 0.03628283739089966 accuracy:  0.9897750020027161\n",
            "200\n",
            "Epoch :  21 for Batch: 3 \t loss : 0.030170584097504616 accuracy:  0.987106204032898\n",
            "200\n",
            "Epoch :  21 for Batch: 4 \t loss : 0.06287931650876999 accuracy:  0.9752936959266663\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  31 for Batch: 0 \t loss : 0.03557615727186203 accuracy:  0.9859249591827393\n",
            "200\n",
            "Epoch :  31 for Batch: 1 \t loss : 0.024632131680846214 accuracy:  0.9898999929428101\n",
            "200\n",
            "Epoch :  31 for Batch: 2 \t loss : 0.0329938568174839 accuracy:  0.989493727684021\n",
            "200\n",
            "Epoch :  31 for Batch: 3 \t loss : 0.029605651274323463 accuracy:  0.987262487411499\n",
            "200\n",
            "Epoch :  31 for Batch: 4 \t loss : 0.06262541562318802 accuracy:  0.9753249883651733\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  41 for Batch: 0 \t loss : 0.03455391898751259 accuracy:  0.9864562153816223\n",
            "200\n",
            "Epoch :  41 for Batch: 1 \t loss : 0.02507673017680645 accuracy:  0.9902937412261963\n",
            "200\n",
            "Epoch :  41 for Batch: 2 \t loss : 0.03486843407154083 accuracy:  0.9897249937057495\n",
            "200\n",
            "Epoch :  41 for Batch: 3 \t loss : 0.032070793211460114 accuracy:  0.9865937232971191\n",
            "200\n",
            "Epoch :  41 for Batch: 4 \t loss : 0.06624302268028259 accuracy:  0.9734500050544739\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  51 for Batch: 0 \t loss : 0.03566966950893402 accuracy:  0.9858812093734741\n",
            "200\n",
            "Epoch :  51 for Batch: 1 \t loss : 0.02526455745100975 accuracy:  0.9899437427520752\n",
            "200\n",
            "Epoch :  51 for Batch: 2 \t loss : 0.03927183151245117 accuracy:  0.9891562461853027\n",
            "200\n",
            "Epoch :  51 for Batch: 3 \t loss : 0.029591241851449013 accuracy:  0.987762451171875\n",
            "200\n",
            "Epoch :  51 for Batch: 4 \t loss : 0.0650646835565567 accuracy:  0.9742187261581421\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  61 for Batch: 0 \t loss : 0.0353550910949707 accuracy:  0.9858749508857727\n",
            "200\n",
            "Epoch :  61 for Batch: 1 \t loss : 0.02394905686378479 accuracy:  0.9902249574661255\n",
            "200\n",
            "Epoch :  61 for Batch: 2 \t loss : 0.030061397701501846 accuracy:  0.9904749989509583\n",
            "200\n",
            "Epoch :  61 for Batch: 3 \t loss : 0.028101397678256035 accuracy:  0.987975001335144\n",
            "200\n",
            "Epoch :  61 for Batch: 4 \t loss : 0.058648284524679184 accuracy:  0.9767062067985535\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  71 for Batch: 0 \t loss : 0.03258118778467178 accuracy:  0.9867499470710754\n",
            "200\n",
            "Epoch :  71 for Batch: 1 \t loss : 0.021712660789489746 accuracy:  0.9912437200546265\n",
            "200\n",
            "Epoch :  71 for Batch: 2 \t loss : 0.027393680065870285 accuracy:  0.9908124804496765\n",
            "200\n",
            "Epoch :  71 for Batch: 3 \t loss : 0.028015833348035812 accuracy:  0.9877437353134155\n",
            "200\n",
            "Epoch :  71 for Batch: 4 \t loss : 0.05819075182080269 accuracy:  0.976256251335144\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  81 for Batch: 0 \t loss : 0.03209434077143669 accuracy:  0.9871875047683716\n",
            "200\n",
            "Epoch :  81 for Batch: 1 \t loss : 0.022013191133737564 accuracy:  0.9911062121391296\n",
            "200\n",
            "Epoch :  81 for Batch: 2 \t loss : 0.025625750422477722 accuracy:  0.9905874729156494\n",
            "200\n",
            "Epoch :  81 for Batch: 3 \t loss : 0.02770846337080002 accuracy:  0.9876749515533447\n",
            "200\n",
            "Epoch :  81 for Batch: 4 \t loss : 0.05773184821009636 accuracy:  0.9762312173843384\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  91 for Batch: 0 \t loss : 0.03164885938167572 accuracy:  0.987387478351593\n",
            "200\n",
            "Epoch :  91 for Batch: 1 \t loss : 0.021043259650468826 accuracy:  0.9913687109947205\n",
            "200\n",
            "Epoch :  91 for Batch: 2 \t loss : 0.026307983323931694 accuracy:  0.9908812046051025\n",
            "200\n",
            "Epoch :  91 for Batch: 3 \t loss : 0.02717156894505024 accuracy:  0.9880874752998352\n",
            "200\n",
            "Epoch :  91 for Batch: 4 \t loss : 0.05502089112997055 accuracy:  0.9774374961853027\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  101 for Batch: 0 \t loss : 0.032656315714120865 accuracy:  0.98685622215271\n",
            "200\n",
            "Epoch :  101 for Batch: 1 \t loss : 0.022733978927135468 accuracy:  0.9909624457359314\n",
            "200\n",
            "Epoch :  101 for Batch: 2 \t loss : 0.027132555842399597 accuracy:  0.9905187487602234\n",
            "200\n",
            "Epoch :  101 for Batch: 3 \t loss : 0.027702603489160538 accuracy:  0.9881437420845032\n",
            "200\n",
            "Epoch :  101 for Batch: 4 \t loss : 0.05734355002641678 accuracy:  0.976699948310852\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  111 for Batch: 0 \t loss : 0.03391331061720848 accuracy:  0.9861562252044678\n",
            "200\n",
            "Epoch :  111 for Batch: 1 \t loss : 0.022748472169041634 accuracy:  0.9905499815940857\n",
            "200\n",
            "Epoch :  111 for Batch: 2 \t loss : 0.026798903942108154 accuracy:  0.9898187518119812\n",
            "200\n",
            "Epoch :  111 for Batch: 3 \t loss : 0.028547003865242004 accuracy:  0.9875437021255493\n",
            "200\n",
            "Epoch :  111 for Batch: 4 \t loss : 0.05809396505355835 accuracy:  0.9756937026977539\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  121 for Batch: 0 \t loss : 0.03368087857961655 accuracy:  0.9865124821662903\n",
            "200\n",
            "Epoch :  121 for Batch: 1 \t loss : 0.024224447086453438 accuracy:  0.9901749491691589\n",
            "200\n",
            "Epoch :  121 for Batch: 2 \t loss : 0.027456508949398994 accuracy:  0.9905437231063843\n",
            "200\n",
            "Epoch :  121 for Batch: 3 \t loss : 0.0320340134203434 accuracy:  0.9866374731063843\n",
            "200\n",
            "Epoch :  121 for Batch: 4 \t loss : 0.064765065908432 accuracy:  0.9742499589920044\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  131 for Batch: 0 \t loss : 0.03293905779719353 accuracy:  0.9867124557495117\n",
            "200\n",
            "Epoch :  131 for Batch: 1 \t loss : 0.022681815549731255 accuracy:  0.9907374978065491\n",
            "200\n",
            "Epoch :  131 for Batch: 2 \t loss : 0.02609105594456196 accuracy:  0.991012454032898\n",
            "200\n",
            "Epoch :  131 for Batch: 3 \t loss : 0.028550777584314346 accuracy:  0.9877499938011169\n",
            "200\n",
            "Epoch :  131 for Batch: 4 \t loss : 0.05843445286154747 accuracy:  0.9762499928474426\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  141 for Batch: 0 \t loss : 0.030423343181610107 accuracy:  0.9876062273979187\n",
            "200\n",
            "Epoch :  141 for Batch: 1 \t loss : 0.020593279972672462 accuracy:  0.991599977016449\n",
            "200\n",
            "Epoch :  141 for Batch: 2 \t loss : 0.02370910905301571 accuracy:  0.9912999868392944\n",
            "200\n",
            "Epoch :  141 for Batch: 3 \t loss : 0.026809072121977806 accuracy:  0.9881374835968018\n",
            "200\n",
            "Epoch :  141 for Batch: 4 \t loss : 0.05371423810720444 accuracy:  0.9774374961853027\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  151 for Batch: 0 \t loss : 0.029865145683288574 accuracy:  0.9877499938011169\n",
            "200\n",
            "Epoch :  151 for Batch: 1 \t loss : 0.020114174112677574 accuracy:  0.9916499853134155\n",
            "200\n",
            "Epoch :  151 for Batch: 2 \t loss : 0.02413586713373661 accuracy:  0.9909186959266663\n",
            "200\n",
            "Epoch :  151 for Batch: 3 \t loss : 0.02610086277127266 accuracy:  0.9883187413215637\n",
            "200\n",
            "Epoch :  151 for Batch: 4 \t loss : 0.05256039276719093 accuracy:  0.9785749912261963\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  161 for Batch: 0 \t loss : 0.029602184891700745 accuracy:  0.9877374768257141\n",
            "200\n",
            "Epoch :  161 for Batch: 1 \t loss : 0.01981145702302456 accuracy:  0.9918562173843384\n",
            "200\n",
            "Epoch :  161 for Batch: 2 \t loss : 0.022734439000487328 accuracy:  0.9913062453269958\n",
            "200\n",
            "Epoch :  161 for Batch: 3 \t loss : 0.026602808386087418 accuracy:  0.988418698310852\n",
            "200\n",
            "Epoch :  161 for Batch: 4 \t loss : 0.05284694954752922 accuracy:  0.9781124591827393\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  171 for Batch: 0 \t loss : 0.030354727059602737 accuracy:  0.9875812530517578\n",
            "200\n",
            "Epoch :  171 for Batch: 1 \t loss : 0.019798245280981064 accuracy:  0.9917937517166138\n",
            "200\n",
            "Epoch :  171 for Batch: 2 \t loss : 0.02406703121960163 accuracy:  0.9908062219619751\n",
            "200\n",
            "Epoch :  171 for Batch: 3 \t loss : 0.02700992114841938 accuracy:  0.9880374670028687\n",
            "200\n",
            "Epoch :  171 for Batch: 4 \t loss : 0.05405987426638603 accuracy:  0.9774187207221985\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  181 for Batch: 0 \t loss : 0.030905650928616524 accuracy:  0.9870124459266663\n",
            "200\n",
            "Epoch :  181 for Batch: 1 \t loss : 0.019827425479888916 accuracy:  0.9917124509811401\n",
            "200\n",
            "Epoch :  181 for Batch: 2 \t loss : 0.02345808781683445 accuracy:  0.9911499619483948\n",
            "200\n",
            "Epoch :  181 for Batch: 3 \t loss : 0.026848355308175087 accuracy:  0.9879812002182007\n",
            "200\n",
            "Epoch :  181 for Batch: 4 \t loss : 0.05358721315860748 accuracy:  0.9777624607086182\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Epoch :  191 for Batch: 0 \t loss : 0.029671911150217056 accuracy:  0.9876624941825867\n",
            "200\n",
            "Epoch :  191 for Batch: 1 \t loss : 0.02064940705895424 accuracy:  0.9914000034332275\n",
            "200\n",
            "Epoch :  191 for Batch: 2 \t loss : 0.024875326082110405 accuracy:  0.9910687208175659\n",
            "200\n",
            "Epoch :  191 for Batch: 3 \t loss : 0.025948097929358482 accuracy:  0.9883437156677246\n",
            "200\n",
            "Epoch :  191 for Batch: 4 \t loss : 0.05268576741218567 accuracy:  0.9783437252044678\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "In this run, another 200 epochs were trained\n",
            "Total epochs model has been trained upon:200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moJiR05IJIv0"
      },
      "source": [
        "**plots the training loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIXgHEy89vn4"
      },
      "source": [
        " print(len(train_losses))\n",
        "iter = np.linspace(0,len(train_losses),len(train_losses))\n",
        "plt.plot(iter,train_losses)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ar-XxVCve6k"
      },
      "source": [
        "**Training the regression, skip these blocks for now**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3oTou4fWELj"
      },
      "source": [
        "MS_criterion= MultiScale(model)\n",
        "EPE_l,MS_l,REC_l=1.5,15,1\n",
        "\n",
        "#the function below is very messy atm, im just happy that it works, it needs to be cleaned up though\n",
        "#it just converts and reshapes tensors to numpy arrays and vica versa during training for the recreational loss\n",
        "\n",
        "#creates the recreation loss, unwarps image and compares with the original image\n",
        "def rec_loss(output_train,x_train,x_train_orig):\n",
        "    output_np = output_train.cpu()\n",
        "    #print(output_np.shape)\n",
        "    \n",
        "    #get shapes of output and convert/reshape into np array\n",
        "    with torch.no_grad():\n",
        "      output_np = np.array(output_np)  \n",
        "    shape = output_np.shape\n",
        "    output_np =np.reshape(output_np,(shape[0],shape[2],shape[3],shape[1]))\n",
        "    \n",
        "\n",
        "    #get shapes of x_train and convert/reshape into np array\n",
        "    x_train_np = x_train.detach().cpu().numpy()\n",
        "    shape2 =x_train_np.shape\n",
        "    x_train_np =np.reshape(x_train_np,(shape2[0],shape2[2],shape2[3],shape2[1]))\n",
        "\n",
        "\n",
        "    #print(x_train_np.shape,type(x_train_np))\n",
        "    x_warped=[]\n",
        "    #undo the warps using prediction flow field and manipulated image\n",
        "    for i in range(train_size):\n",
        "      x_warped.append(warp(x_train_np[i],output_np[i]))\n",
        "    #convert from torch to numpy and reshape\n",
        "    x_warped = np.array(x_warped)\n",
        "    shape_warp = x_warped.shape\n",
        "\n",
        "    #get unwarped image and turn it into torch format\n",
        "    x_warped = np.reshape(x_warped,(shape_warp[0],shape_warp[3],shape_warp[1],shape_warp[2]))\n",
        "    x_warp_tens = torch.from_numpy(x_warped).float()\n",
        "    #turn it into cuda tensor\n",
        "    if torch.cuda.is_available():\n",
        "      x_warp_tens = x_warp_tens.cuda()\n",
        "    #return the norm of the warped image-original image as the loss\n",
        "    return torch.norm(x_warp_tens-x_train_orig,p=2,dim=1).mean()\n",
        "\n",
        "\n",
        "\n",
        "def train_regress(epoch,EPE_l,MS_l,REC_l):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    # getting the training set\n",
        "    x_train, y_train = Variable(train_x), Variable(train_y)\n",
        "    x_train_orig=Variable(train_x_orig)\n",
        "    # converting the data into GPU format\n",
        "    if torch.cuda.is_available():\n",
        "        x_train = x_train.cuda()\n",
        "        y_train = y_train.cuda()\n",
        "        x_train_orig=x_train_orig.cuda()  \n",
        "    # clearing the Gradients of the model parameters\n",
        "    optimizer.zero_grad()\n",
        "    # prediction for training and validation set\n",
        "    output_train = model(x_train)\n",
        "    print(\"output train shape\",output_train.shape)\n",
        "    print(output_train)\n",
        "    total = train_size\n",
        "    # computing the training and validation loss\n",
        "    #print(x_train.shape)\n",
        "    #print(output_train.shape)\n",
        "    print(\"y train shape\",y_train.shape)\n",
        "    print(y_train)\n",
        "    REC_loss = rec_loss(output_train,x_train,x_train_orig)\n",
        "    EPE_loss_train = EPE(output_train,y_train.float())\n",
        "    MS_loss_train,_ = MS_criterion(output_train, y_train.float())\n",
        "    \n",
        "    loss_train=EPE_l*EPE_loss_train+MS_l*MS_loss_train +REC_l*REC_loss \n",
        "    train_losses.append(loss_train)\n",
        "    \n",
        "    # computing the updated weights of all the model parameters\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    tr_loss = loss_train.item()\n",
        "\n",
        "    if epoch%10 == 0:\n",
        "        # printing the validation loss\n",
        "        print('Epoch : ',epoch+1, '\\t', 'loss :',loss_train.item(),\"accuracy: \")\n",
        "        print(\"epe: \",EPE_loss_train.item(),\" MS: \",MS_loss_train.item(),\" reconstruct: \",REC_loss.item())\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy-SmfT1srny"
      },
      "source": [
        "n_epochs = 500\n",
        "train_losses =[]\n",
        "correct =0\n",
        "#same parameters the FAL paper have used\n",
        "optimizer = Adam(model.parameters(),lr=0.0001,betas =(0.9,0.999)) \n",
        "for epoch in range(n_epochs): \n",
        "    train_regress(epoch,EPE_l,MS_l,REC_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn7gOX8Usrny"
      },
      "source": [
        "#function to\n",
        "def get_heatmap_cv(img, magn, max_flow_mag):\n",
        "    min_flow_mag = .5\n",
        "    cv_magn = np.clip(\n",
        "        255 * (magn - min_flow_mag) / (max_flow_mag - min_flow_mag),\n",
        "        a_min=0,\n",
        "        a_max=255).astype(np.uint8)\n",
        "    if img.dtype != np.uint8:\n",
        "        img = (255 * img).astype(np.uint8)\n",
        "        \n",
        "    heatmap_img = cv2.applyColorMap(cv_magn, cv2.COLORMAP_MAGMA)\n",
        "    heatmap_img = heatmap_img[..., ::-1]\n",
        "    h, w = magn.shape\n",
        "    img_alpha = np.ones((h, w), dtype=np.double)[:, :, None]\n",
        "    heatmap_alpha = np.clip(\n",
        "        magn / max_flow_mag, a_min=0, a_max=1)[:, :, None]**.7\n",
        "    heatmap_alpha[heatmap_alpha < .2]**.5\n",
        "    pm_hm = heatmap_img * heatmap_alpha\n",
        "    pm_img = img * img_alpha\n",
        "    cv_out = pm_hm + pm_img * (1 - heatmap_alpha)\n",
        "    cv_out = np.clip(cv_out, a_min=0, a_max=255).astype(np.uint8)\n",
        "    return cv_out\n",
        "\n",
        "def save_heatmap_cv(img, magn, path, max_flow_mag=2):\n",
        "    cv_out = get_heatmap_cv(img, magn, max_flow_mag)\n",
        "    out = Image.fromarray(cv_out)\n",
        "    plt.imshow(out)\n",
        "    plt.show()\n",
        "    out.save(path, quality=95)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQOqmEbmsrny"
      },
      "source": [
        "from utils.tools import *\n",
        "from utils.visualize import *\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "tf = transforms.Compose([transforms.ToTensor(),\n",
        "                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAdi8lECsrny"
      },
      "source": [
        "#quick sanity check again\n",
        "\n",
        "#print(flow.shape)\n",
        "#print(pic.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM2lLcMnIcha"
      },
      "source": [
        "**uses training image to predict the flow field**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wg4-nudsrny"
      },
      "source": [
        "test_ref = X_ref[2]\n",
        "test_mod = X_mod[2]\n",
        "ground_flow = cv2.calcOpticalFlowFarneback(cv2.cvtColor(test_ref,cv2.COLOR_BGR2GRAY), cv2.cvtColor(test_mod,cv2.COLOR_BGR2GRAY),None, 0.5, 3, 4, 2, 3, 1.2, 0)\n",
        "image = np.array([test_mod])\n",
        "\n",
        "shapes = image.shape\n",
        "\n",
        "image = np.reshape(image,(shapes[0],shapes[3],shapes[1],shapes[2]))\n",
        "image_tensor = torch.from_numpy(image)\n",
        "image_tensor= Variable(image_tensor)\n",
        "# converting the data into GPU format\n",
        "if torch.cuda.is_available():\n",
        "    image_tensor = image_tensor.cuda()\n",
        "with torch.no_grad():\n",
        "        flow = perPix_model(image_tensor.float()).float().cpu()\n",
        "        print(flow.shape)\n",
        "        flow = torch.argmax(flow,dim =1).numpy()\n",
        "        flow = np.reshape(flow,(shapes[1],shapes[2]))\n",
        "        \n",
        "        print(flow)\n",
        "        real_flow = np.zeros((2,400,400))\n",
        "        temp =0\n",
        "        for i in range(shapes[1]):\n",
        "          for j in range(shapes[2]):\n",
        "            real_flow[:,i,j]=list(dic.keys())[list(dic.values()).index(flow[i][j])]\n",
        "        print(real_flow.shape)\n",
        "        np.set_printoptions(edgeitems=3)\n",
        "        #print(real_flow)\n",
        "        real_flow = np.transpose(real_flow, (1, 2, 0))\n",
        "        print(real_flow.shape)\n",
        "        #print(real_flow)\n",
        "        w, h, _ = real_flow.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH_Cwm4Psrny"
      },
      "source": [
        "image = np.reshape(image,(h,w,3))\n",
        "image = Image.fromarray(image,\"RGB\")\n",
        "modified = image.resize((h, w),Image.BICUBIC)\n",
        "modified_np = np.asarray(modified)\n",
        "flow = flow_resize(real_flow, modified.size)\n",
        "#print(flow.shape)\n",
        "mag, ang = cv2.cartToPolar(flow[...,1], flow[...,0])\n",
        "#print(mag.shape)\n",
        "#print(modified_np.shape)\n",
        "#save your image to the folder\n",
        "path = \"tester.png\"\n",
        "tester =get_heatmap_cv(modified_np,mag,max_flow_mag=10)\n",
        "plt.title(\"Predicted modification\")\n",
        "plt.imshow(tester)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI3Yagn9IznO"
      },
      "source": [
        "**plots the ground truth heatmap on the image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AbiG2ez9tzm"
      },
      "source": [
        "\n",
        "mag, ang = cv2.cartToPolar(ground_flow[...,0], ground_flow[...,1])\n",
        "#print(mag.shape)\n",
        "real = get_heatmap_cv(modified_np,mag,max_flow_mag=12)\n",
        "plt.title(\"Ground truth modification\")\n",
        "plt.imshow(real)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26P3Me_CGm9H"
      },
      "source": [
        "~"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}